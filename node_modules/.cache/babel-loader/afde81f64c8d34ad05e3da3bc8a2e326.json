{"ast":null,"code":"var _jsxFileName = \"/Users/almazhan/Desktop/sonyc-app-1/src/pages/DataVisualization.js\";\nimport React from 'react';\nimport Header from '../components/Header';\nimport Footer from '../components/Footer';\nimport lit1 from '../images/lit1.png';\nimport lit2 from '../images/lit2.png';\nimport lit3 from '../images/lit3.png';\nimport lit4 from '../images/lit4.png';\nimport lit5 from '../images/lit5.png';\nimport granular from '../images/granular.png';\nimport './DataVisualization.css';\nimport Collapsible from 'react-collapsible';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nimport { Fragment as _Fragment } from \"react/jsx-dev-runtime\";\n\nfunction DataVisualization() {\n  return /*#__PURE__*/_jsxDEV(_Fragment, {\n    children: [/*#__PURE__*/_jsxDEV(Header, {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 16,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"home\",\n      children: [/*#__PURE__*/_jsxDEV(\"h2\", {\n        className: \"header-text\",\n        children: \"Data Visualization\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 18,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"block\",\n        children: [/*#__PURE__*/_jsxDEV(\"p\", {\n          className: \"full-text\",\n          children: [/*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 21,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 22,\n            columnNumber: 13\n          }, this), \"Sensors are used to measure air and noise pollution levels and visualizing the data is the first step in making decisions about the data.\"]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 20,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 27,\n          columnNumber: 11\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 19,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 29,\n        columnNumber: 9\n      }, this), \" \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 29,\n        columnNumber: 16\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"block\",\n        children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n          className: \"gif-text\",\n          children: \"Data Visualization Research Review\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 31,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n          children: [/*#__PURE__*/_jsxDEV(\"p\", {\n            className: \"full-text\",\n            children: [/*#__PURE__*/_jsxDEV(\"b\", {\n              children: [' ', \"Guess the Data: Data Work to Understand How People Make Sense of and Use Simple Sensor Data from Homes .Albrecht Kurze, Andreas Bischof, S\\xF6ren Totzauer, Michael Storz, Maximilian Eibl, Margot Brereton, and Arne Berger. 2020.\"]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 35,\n              columnNumber: 15\n            }, this), /*#__PURE__*/_jsxDEV(\"i\", {\n              children: \"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 42,\n              columnNumber: 15\n            }, this), \". Association for Computing Machinery, New York, NY, USA, 1\\u201312. DOI:https://doi.org/10.1145/3313831.3376273\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 34,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(Collapsible, {\n            trigger: \"Read More\",\n            className: \"full-text\",\n            children: [/*#__PURE__*/_jsxDEV(\"p\", {\n              className: \"full-text\",\n              children: [\"The authors investigate human sensemaking of such sensor data can reveal domestic activities and to achieve that task, develop and field-test the Guess the data method,which enabled people to use and make sense of live data from their homes and to collectively interpret and reflect on anonymized data from the homes in the study. The authors decided to use simple line graphs as data visualizations for temperature, light level, humidity, barometric pressure and movement (accelerometer values). They wanted to undertake very little pre-processing, presenting close to \\u2018raw\\u2019 data, to prevent interpretation bias. Such simple time series graphs are comparable to those used in other cited studies. The findings show how participants reconstruct behavior, both individually and collectively, expose the sensitive personal data of others, and use sensor data as evidence and for lateral surveillance within the household.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 66,\n                columnNumber: 17\n              }, this), \"Visualized sensor data used as evidence and proof: Participants used visualized data to confirm their assumptions about other residents\\u2019 behavior, not only retrospectively during discussions but also pro-actively during data collection when they had access to it. For example, a participant corrected the partner\\u2019s careless behavior regarding the light in the hallway (figure 3), which he often forgot to turn off. She confronted him with the visualization of the light sensor data, and \\u201Che was a little bit shocked\\u201D (II.A)\"]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 50,\n              columnNumber: 15\n            }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n              src: lit1,\n              alt: \"poster gif\",\n              className: \"vis-img\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 77,\n              columnNumber: 15\n            }, this)]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 49,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 79,\n            columnNumber: 13\n          }, this)]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 33,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n          children: [/*#__PURE__*/_jsxDEV(\"p\", {\n            className: \"full-text\",\n            children: [/*#__PURE__*/_jsxDEV(\"b\", {\n              children: \"Nicolas Maisonneuve, Matthias Stevens, Maria E. Niessen, Peter Hanappe, and Luc Steels. 2009. Citizen noise pollution monitoring.\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 84,\n              columnNumber: 15\n            }, this), \"In\", ' ', /*#__PURE__*/_jsxDEV(\"i\", {\n              children: \"Proceedings of the 10th Annual International Conference on Digital Government Research: Social Networks: Making Connections between Citizens, Data and Government\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 90,\n              columnNumber: 15\n            }, this), ' ', \"(\", /*#__PURE__*/_jsxDEV(\"i\", {\n              children: \"dg.o '09\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 95,\n              columnNumber: 16\n            }, this), \"). Digital Government Society of North America, 96\\u2013103. https://dl.acm.org/doi/10.5555/1556176.1556198\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 83,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(Collapsible, {\n            trigger: \"Read More\",\n            className: \"full-text\",\n            children: /*#__PURE__*/_jsxDEV(\"p\", {\n              className: \"full-text\",\n              children: [\"In this paper, authors present a new approach to monitor noise pollution involving citizens who can measure their personal exposure to noise in their everyday environment by using GPS-equipped mobile phones as noise sensors. The geo-localised measures and user-generated meta-data can be automatically sent and shared online with the public to contribute to the collective noise mapping of cities. The prototype called Noise Tube can be found online. This application collects local information from different sensors (noise level, GPS coordinates, time, user input) and sends them to the NoiseTube server which visualizes the noise level data. The server centralises and processes the data sent by the phones.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 112,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 113,\n                columnNumber: 17\n              }, this), \"The mobile application contains a real-time signal processing algorithm which measures the loudness level of the microphone recording the environmental sound (at 22500 Hz, 16 bits) over 1 second at a chosen interval. On top of the sensing of the loudness a real time visualization is displayed on the phone with the decibels. To add meaning to this value it is associated with a colour that represents the health risk of the current exposure level: less than 70: green (no risk); between 70 and 80: yellow (be careful); more than 80: red (risky). See figure 2 below.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 124,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 125,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n                src: lit2,\n                alt: \"poster gif\",\n                className: \"vis-img\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 126,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 127,\n                columnNumber: 17\n              }, this), \"In addition to measured loudness, the app allows to record the source or context of noise, which is not always available but remains important. Especially because the appreciation of sound and loudness is a subjective matter \\u2013 i.e. the perceived annoyance (or pleasure) does not always correlate with its loudness (see 6.2). Context is recorded through environmental tagging (source of a noise e.g.: cars, aircraft, neighbours and an annoyance rating/tag) and geo-tagging (gps positioning or place tags (such as \\u201Chome\\u201D, \\u201Cwork\\u201D, the name of the subway station, ...) Geo-tagging feature we can reconstruct the geo-coordinates afterwards notably for indoor locations (cf. subway noise map in figure 3).\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 140,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n                src: lit3,\n                alt: \"poster gif\",\n                className: \"vis-img\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 141,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 142,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 143,\n                columnNumber: 17\n              }, this), \"Visualising Noise Maps. Once the measured data is sent to the server, any user can see his own contributions or exposures by going to the NoiseTube website and visualizing them on a map thanks to Google Earth. The collective noise map is also publicly available constructed by aggregating all the shared participants. Each map can show a layer of participants to add context and meaning to the loudness data. The authors also allow users embed this as a web widget into their personal web pages and provide publicly accessible web API to give full access to third parties such scientists or developers can use individual or collective exposure data to create web mash-ups or analyse data for scientific purposes.\"]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 99,\n              columnNumber: 15\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 98,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 158,\n            columnNumber: 13\n          }, this)]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 82,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n          children: [/*#__PURE__*/_jsxDEV(\"p\", {\n            className: \"full-text\",\n            children: [/*#__PURE__*/_jsxDEV(\"b\", {\n              children: [\"Silvia Santini, Benedikt Ostermaier, and Andrea Vitaletti. 2008. First experiences using wireless sensor networks for noise pollution monitoring.\", ' ']\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 162,\n              columnNumber: 15\n            }, this), \"In\", ' ', /*#__PURE__*/_jsxDEV(\"i\", {\n              children: \"Proceedings of the workshop on Real-world wireless sensor networks\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 168,\n              columnNumber: 15\n            }, this), ' ', \"(\", /*#__PURE__*/_jsxDEV(\"i\", {\n              children: \"REALWSN '08\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 172,\n              columnNumber: 16\n            }, this), \"). Association for Computing Machinery, New York, NY, USA, 61\\u201365. DOI:https://doi.org/10.1145/1435473.1435490\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 161,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(Collapsible, {\n            trigger: \"Read More\",\n            className: \"full-text\",\n            children: /*#__PURE__*/_jsxDEV(\"p\", {\n              className: \"full-text\",\n              children: [\"The authors focus on the assessment of environmental noise pollution in urban areas and provide a feasibility analysis of wireless sensor networks. They also present a prototype for the collection and logging of noise pollution data based on the Tmote invent prototyping platform, using which they performed indoor and outdoor noise pollution measurements. They also present tinyLAB, a Matlab-based tool developed in the context of this work, which enables real-time acquisition, processing and visualization of data collected in wireless sensor networks. Authors mention that prototyping wireless sensor network applications often requires visualizing the sensor data to quickly identify any malfunctioning. For example, figure 1 shows the responses to these acoustic events of four different nodes, clearly pointing out a misalignment in the measured equivalent noise levels. \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 191,\n                columnNumber: 31\n              }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 192,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n                src: lit4,\n                alt: \"poster gif\",\n                className: \"vis-img\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 193,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 194,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 195,\n                columnNumber: 17\n              }, this), \"Additionally, authors mention that current tools often do not provide satisfactory data processing and visualization features and propose using Matlab as it serves scientists in managing, processing and visualizing their data and appears therefore particularly well-suited to be used in the context of wireless sensor networks. Authors develop tinyLAB, a simple framework that allows to receive and send messages from and to a sensor network and to visualize and process data as it comes from the network. tinyLAB is implemented relying solely on the Matlab software suite and offers a simple API to receive and send data.\"]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 176,\n              columnNumber: 15\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 175,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 208,\n            columnNumber: 13\n          }, this)]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 160,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n          children: [/*#__PURE__*/_jsxDEV(\"p\", {\n            className: \"full-text\",\n            children: [/*#__PURE__*/_jsxDEV(\"b\", {\n              children: [\"Ann-sofie Gunnarsson, Malinda Rauhala, Anders Henrysson, and Anders Ynnerman. 2006. Visualization of sensor data using mobile phone augmented reality.\", ' ']\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 212,\n              columnNumber: 15\n            }, this), \"In\", ' ', /*#__PURE__*/_jsxDEV(\"i\", {\n              children: \"Proceedings of the 5th IEEE and ACM International Symposium on Mixed and Augmented Reality\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 218,\n              columnNumber: 15\n            }, this), ' ', \"(\", /*#__PURE__*/_jsxDEV(\"i\", {\n              children: \"ISMAR '06\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 222,\n              columnNumber: 16\n            }, this), \"). IEEE Computer Society, USA, 233\\u2013234. DOI:https://doi.org/10.1109/ISMAR.2006.297820\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 211,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(Collapsible, {\n            trigger: \"Read More\",\n            className: \"full-text\",\n            children: /*#__PURE__*/_jsxDEV(\"p\", {\n              className: \"full-text\",\n              children: [\"The authors developed a prototype system for visual inspection of hidden structures using a mobile phone wireless ZigBee sensor network. Data collected from an embedded wireless sensor matrix is used to synthesize AR visualizations in real-time. The AR visualization is providing the user with an instant insight concerning the status of the element being augmented. The authors arrange sensors in a grid (e.g. a matrix), which allows to take an approach where individual sensors emerge as pixels in an image when their data is translated into color values. See below an image that shows a mobile application overview.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 237,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n                src: lit5,\n                alt: \"poster gif\",\n                className: \"vis-img\"\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 238,\n                columnNumber: 17\n              }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 239,\n                columnNumber: 17\n              }, this), \" \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 239,\n                columnNumber: 24\n              }, this), \"Authors argue that AR is an ideal way to present such context related visualizations since it eliminates the focus switching between the visualization domain (image) and problem domain (real world). The sensors measure the relative humidity (RH) at the location of the sensor, providing with data from a discrete set of measure points in 3D. The values between the measure points are interpolated creating a continuous visualization which provides the user an overview of the humidity values as well as their distribution. The mobile phone application contains a visualization engine and a communication layer. The interpolation is performed in real time and every time a new sensor value is retrieved from the sensor network the visualization is updated. Two visualization options are implemented, one fully continuous, see Figure below, while the other is composed of small quadratic units, separated using full transparency. Authors believe that the user experiences a better sense of orientation when less background information is covered by the visualization.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 258,\n                columnNumber: 17\n              }, this)]\n            }, void 0, true, {\n              fileName: _jsxFileName,\n              lineNumber: 226,\n              columnNumber: 15\n            }, this)\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 225,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 261,\n            columnNumber: 13\n          }, this)]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 210,\n          columnNumber: 11\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 30,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 264,\n        columnNumber: 9\n      }, this), \" \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 264,\n        columnNumber: 16\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"block\",\n        children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n          className: \"gif-text\",\n          children: \"Sensor Data Visualization Challenges \"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 266,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n          className: \"full-text\",\n          children: [\"Visualizing large amounts of temporal data requires balancing the goals of achieving high performance and interactivity. One solution lies in intelligently aggregating the data to higher granularities, so that the number of data points to be visualized is reduced and is easier for the user to interpret - view image below. \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 272,\n            columnNumber: 66\n          }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n            src: lit1,\n            alt: \"poster gif\",\n            className: \"vis-img\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 273,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 274,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 275,\n            columnNumber: 13\n          }, this), \"Moreover, visualizing the longitudinal data such as noise or air pollution data is challenging due to continuity of the data (no precise start and end). For example, Many line or bar charts that deal with the 24-hour cycle simply pick a point at which the chart starts and ends. Sometimes the charts go from 12am-12am, sometimes they use ranges like 4am-4am (which puts the break during a time when most people are sleeping). For specific data this is often acceptable, but in general it is a limitation (How can you pick an arbitrary time to break the data? How are you sure the most interesting part of the data doesn\\u2019t overlap when the chart begins and ends?)\"]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 267,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 288,\n          columnNumber: 11\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 265,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 290,\n        columnNumber: 9\n      }, this), \" \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 290,\n        columnNumber: 16\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"block\",\n        children: [/*#__PURE__*/_jsxDEV(\"h4\", {\n          className: \"gif-text\",\n          children: \"Visualization Graphs and Discussion \"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 292,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"h6\", {\n          className: \"gif-text-h6\",\n          children: \"Line Chart \"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 293,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n          className: \"full-text\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 294,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 295,\n          columnNumber: 11\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 291,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 297,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 17,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(Footer, {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 299,\n      columnNumber: 7\n    }, this)]\n  }, void 0, true);\n}\n\n_c = DataVisualization;\nexport default DataVisualization;\n\nvar _c;\n\n$RefreshReg$(_c, \"DataVisualization\");","map":{"version":3,"sources":["/Users/almazhan/Desktop/sonyc-app-1/src/pages/DataVisualization.js"],"names":["React","Header","Footer","lit1","lit2","lit3","lit4","lit5","granular","Collapsible","DataVisualization"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAOC,MAAP,MAAmB,sBAAnB;AACA,OAAOC,MAAP,MAAmB,sBAAnB;AACA,OAAOC,IAAP,MAAiB,oBAAjB;AACA,OAAOC,IAAP,MAAiB,oBAAjB;AACA,OAAOC,IAAP,MAAiB,oBAAjB;AACA,OAAOC,IAAP,MAAiB,oBAAjB;AACA,OAAOC,IAAP,MAAiB,oBAAjB;AACA,OAAOC,QAAP,MAAqB,wBAArB;AACA,OAAO,yBAAP;AACA,OAAOC,WAAP,MAAwB,mBAAxB;;;;AAEA,SAASC,iBAAT,GAA6B;AAC3B,sBACE;AAAA,4BACE,QAAC,MAAD;AAAA;AAAA;AAAA;AAAA,YADF,eAEE;AAAK,MAAA,SAAS,EAAC,MAAf;AAAA,8BACE;AAAI,QAAA,SAAS,EAAC,aAAd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cADF,eAEE;AAAK,QAAA,SAAS,EAAC,OAAf;AAAA,gCACE;AAAG,UAAA,SAAS,EAAC,WAAb;AAAA,kCACE;AAAA;AAAA;AAAA;AAAA,kBADF,eAEE;AAAA;AAAA;AAAA;AAAA,kBAFF;AAAA;AAAA;AAAA;AAAA;AAAA,gBADF,eAQE;AAAA;AAAA;AAAA;AAAA,gBARF;AAAA;AAAA;AAAA;AAAA;AAAA,cAFF,eAYE;AAAA;AAAA;AAAA;AAAA,cAZF,oBAYS;AAAA;AAAA;AAAA;AAAA,cAZT,eAaE;AAAK,QAAA,SAAS,EAAC,OAAf;AAAA,gCACE;AAAI,UAAA,SAAS,EAAC,UAAd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBADF,eAGE;AAAA,kCACE;AAAG,YAAA,SAAS,EAAC,WAAb;AAAA,oCACE;AAAA,yBACG,GADH;AAAA;AAAA;AAAA;AAAA;AAAA,oBADF,eAQE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBARF;AAAA;AAAA;AAAA;AAAA;AAAA,kBADF,eAgBE,QAAC,WAAD;AAAa,YAAA,OAAO,EAAC,WAArB;AAAiC,YAAA,SAAS,EAAC,WAA3C;AAAA,oCACE;AAAG,cAAA,SAAS,EAAC,WAAb;AAAA,28BAgBE;AAAA;AAAA;AAAA;AAAA,sBAhBF;AAAA;AAAA;AAAA;AAAA;AAAA,oBADF,eA4BE;AAAK,cAAA,GAAG,EAAEP,IAAV;AAAgB,cAAA,GAAG,EAAC,YAApB;AAAiC,cAAA,SAAS,EAAC;AAA3C;AAAA;AAAA;AAAA;AAAA,oBA5BF;AAAA;AAAA;AAAA;AAAA;AAAA,kBAhBF,eA8CE;AAAA;AAAA;AAAA;AAAA,kBA9CF;AAAA;AAAA;AAAA;AAAA;AAAA,gBAHF,eAoDE;AAAA,kCACE;AAAG,YAAA,SAAS,EAAC,WAAb;AAAA,oCACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBADF,QAMK,GANL,eAOE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAPF,EAWO,GAXP,oBAYG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAZH;AAAA;AAAA;AAAA;AAAA;AAAA,kBADF,eAgBE,QAAC,WAAD;AAAa,YAAA,OAAO,EAAC,WAArB;AAAiC,YAAA,SAAS,EAAC,WAA3C;AAAA,mCACE;AAAG,cAAA,SAAS,EAAC,WAAb;AAAA,gvBAaE;AAAA;AAAA;AAAA;AAAA,sBAbF,eAcE;AAAA;AAAA;AAAA;AAAA,sBAdF,ykBAyBE;AAAA;AAAA;AAAA;AAAA,sBAzBF,eA0BE;AAAA;AAAA;AAAA;AAAA,sBA1BF,eA2BE;AAAK,gBAAA,GAAG,EAAEC,IAAV;AAAgB,gBAAA,GAAG,EAAC,YAApB;AAAiC,gBAAA,SAAS,EAAC;AAA3C;AAAA;AAAA;AAAA;AAAA,sBA3BF,eA4BE;AAAA;AAAA;AAAA;AAAA,sBA5BF,uuBAyCE;AAAA;AAAA;AAAA;AAAA,sBAzCF,eA0CE;AAAK,gBAAA,GAAG,EAAEC,IAAV;AAAgB,gBAAA,GAAG,EAAC,YAApB;AAAiC,gBAAA,SAAS,EAAC;AAA3C;AAAA;AAAA;AAAA;AAAA,sBA1CF,eA2CE;AAAA;AAAA;AAAA;AAAA,sBA3CF,eA4CE;AAAA;AAAA;AAAA;AAAA,sBA5CF;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,kBAhBF,eA4EE;AAAA;AAAA;AAAA;AAAA,kBA5EF;AAAA;AAAA;AAAA;AAAA;AAAA,gBApDF,eAkIE;AAAA,kCACE;AAAG,YAAA,SAAS,EAAC,WAAb;AAAA,oCACE;AAAA,8KAGwB,GAHxB;AAAA;AAAA;AAAA;AAAA;AAAA,oBADF,QAMK,GANL,eAOE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAPF,EAUO,GAVP,oBAWG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAXH;AAAA;AAAA;AAAA;AAAA;AAAA,kBADF,eAeE,QAAC,WAAD;AAAa,YAAA,OAAO,EAAC,WAArB;AAAiC,YAAA,SAAS,EAAC,WAA3C;AAAA,mCACE;AAAG,cAAA,SAAS,EAAC,WAAb;AAAA,w5BAegB;AAAA;AAAA;AAAA;AAAA,sBAfhB,eAgBE;AAAA;AAAA;AAAA;AAAA,sBAhBF,eAiBE;AAAK,gBAAA,GAAG,EAAEC,IAAV;AAAgB,gBAAA,GAAG,EAAC,YAApB;AAAiC,gBAAA,SAAS,EAAC;AAA3C;AAAA;AAAA;AAAA;AAAA,sBAjBF,eAkBE;AAAA;AAAA;AAAA;AAAA,sBAlBF,eAmBE;AAAA;AAAA;AAAA;AAAA,sBAnBF;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,kBAfF,eAgDE;AAAA;AAAA;AAAA;AAAA,kBAhDF;AAAA;AAAA;AAAA;AAAA;AAAA,gBAlIF,eAoLE;AAAA,kCACE;AAAG,YAAA,SAAS,EAAC,WAAb;AAAA,oCACE;AAAA,mLAG2B,GAH3B;AAAA;AAAA;AAAA;AAAA;AAAA,oBADF,QAMK,GANL,eAOE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAPF,EAUO,GAVP,oBAWG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAXH;AAAA;AAAA;AAAA;AAAA;AAAA,kBADF,eAeE,QAAC,WAAD;AAAa,YAAA,OAAO,EAAC,WAArB;AAAiC,YAAA,SAAS,EAAC,WAA3C;AAAA,mCACE;AAAG,cAAA,SAAS,EAAC,WAAb;AAAA,opBAWE;AAAA;AAAA;AAAA;AAAA,sBAXF,eAYE;AAAK,gBAAA,GAAG,EAAEC,IAAV;AAAgB,gBAAA,GAAG,EAAC,YAApB;AAAiC,gBAAA,SAAS,EAAC;AAA3C;AAAA;AAAA;AAAA;AAAA,sBAZF,eAaE;AAAA;AAAA;AAAA;AAAA,sBAbF,oBAaS;AAAA;AAAA;AAAA;AAAA,sBAbT,4jCAgCE;AAAA;AAAA;AAAA;AAAA,sBAhCF;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,kBAfF,eAmDE;AAAA;AAAA;AAAA;AAAA,kBAnDF;AAAA;AAAA;AAAA;AAAA;AAAA,gBApLF;AAAA;AAAA;AAAA;AAAA;AAAA,cAbF,eAuPE;AAAA;AAAA;AAAA;AAAA,cAvPF,oBAuPS;AAAA;AAAA;AAAA;AAAA,cAvPT,eAwPE;AAAK,QAAA,SAAS,EAAC,OAAf;AAAA,gCACE;AAAI,UAAA,SAAS,EAAC,UAAd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBADF,eAEE;AAAG,UAAA,SAAS,EAAC,WAAb;AAAA,0WAKuD;AAAA;AAAA;AAAA;AAAA,kBALvD,eAME;AAAK,YAAA,GAAG,EAAEJ,IAAV;AAAgB,YAAA,GAAG,EAAC,YAApB;AAAiC,YAAA,SAAS,EAAC;AAA3C;AAAA;AAAA;AAAA;AAAA,kBANF,eAOE;AAAA;AAAA;AAAA;AAAA,kBAPF,eAQE;AAAA;AAAA;AAAA;AAAA,kBARF;AAAA;AAAA;AAAA;AAAA;AAAA,gBAFF,eAuBE;AAAA;AAAA;AAAA;AAAA,gBAvBF;AAAA;AAAA;AAAA;AAAA;AAAA,cAxPF,eAiRE;AAAA;AAAA;AAAA;AAAA,cAjRF,oBAiRS;AAAA;AAAA;AAAA;AAAA,cAjRT,eAkRE;AAAK,QAAA,SAAS,EAAC,OAAf;AAAA,gCACE;AAAI,UAAA,SAAS,EAAC,UAAd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBADF,eAEE;AAAI,UAAA,SAAS,EAAC,aAAd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAFF,eAGE;AAAG,UAAA,SAAS,EAAC;AAAb;AAAA;AAAA;AAAA;AAAA,gBAHF,eAIE;AAAA;AAAA;AAAA;AAAA,gBAJF;AAAA;AAAA;AAAA;AAAA;AAAA,cAlRF,eAwRE;AAAA;AAAA;AAAA;AAAA,cAxRF;AAAA;AAAA;AAAA;AAAA;AAAA,YAFF,eA4RE,QAAC,MAAD;AAAA;AAAA;AAAA;AAAA,YA5RF;AAAA,kBADF;AAgSD;;KAjSQO,iB;AAmST,eAAeA,iBAAf","sourcesContent":["import React from 'react'\nimport Header from '../components/Header'\nimport Footer from '../components/Footer'\nimport lit1 from '../images/lit1.png'\nimport lit2 from '../images/lit2.png'\nimport lit3 from '../images/lit3.png'\nimport lit4 from '../images/lit4.png'\nimport lit5 from '../images/lit5.png'\nimport granular from '../images/granular.png'\nimport './DataVisualization.css'\nimport Collapsible from 'react-collapsible'\n\nfunction DataVisualization() {\n  return (\n    <>\n      <Header />\n      <div className=\"home\">\n        <h2 className=\"header-text\">Data Visualization</h2>\n        <div className=\"block\">\n          <p className=\"full-text\">\n            <br />\n            <br />\n            Sensors are used to measure air and noise pollution levels and\n            visualizing the data is the first step in making decisions about the\n            data.\n          </p>\n          <br />\n        </div>\n        <br /> <br />\n        <div className=\"block\">\n          <h4 className=\"gif-text\">Data Visualization Research Review</h4>\n\n          <div>\n            <p className=\"full-text\">\n              <b>\n                {' '}\n                Guess the Data: Data Work to Understand How People Make Sense of\n                and Use Simple Sensor Data from Homes .Albrecht Kurze, Andreas\n                Bischof, Sören Totzauer, Michael Storz, Maximilian Eibl, Margot\n                Brereton, and Arne Berger. 2020.\n              </b>\n              <i>\n                Proceedings of the 2020 CHI Conference on Human Factors in\n                Computing Systems\n              </i>\n              . Association for Computing Machinery, New York, NY, USA, 1–12.\n              DOI:https://doi.org/10.1145/3313831.3376273\n            </p>\n            <Collapsible trigger=\"Read More\" className=\"full-text\">\n              <p className=\"full-text\">\n                The authors investigate human sensemaking of such sensor data\n                can reveal domestic activities and to achieve that task, develop\n                and field-test the Guess the data method,which enabled people to\n                use and make sense of live data from their homes and to\n                collectively interpret and reflect on anonymized data from the\n                homes in the study. The authors decided to use simple line\n                graphs as data visualizations for temperature, light level,\n                humidity, barometric pressure and movement (accelerometer\n                values). They wanted to undertake very little pre-processing,\n                presenting close to ‘raw’ data, to prevent interpretation bias.\n                Such simple time series graphs are comparable to those used in\n                other cited studies. The findings show how participants\n                reconstruct behavior, both individually and collectively, expose\n                the sensitive personal data of others, and use sensor data as\n                evidence and for lateral surveillance within the household.\n                <br />\n                Visualized sensor data used as evidence and proof: Participants\n                used visualized data to confirm their assumptions about other\n                residents’ behavior, not only retrospectively during discussions\n                but also pro-actively during data collection when they had\n                access to it. For example, a participant corrected the partner’s\n                careless behavior regarding the light in the hallway (figure 3),\n                which he often forgot to turn off. She confronted him with the\n                visualization of the light sensor data, and “he was a little bit\n                shocked” (II.A)\n              </p>\n              <img src={lit1} alt=\"poster gif\" className=\"vis-img\" />\n            </Collapsible>\n            <br />\n          </div>\n\n          <div>\n            <p className=\"full-text\">\n              <b>\n                Nicolas Maisonneuve, Matthias Stevens, Maria E. Niessen, Peter\n                Hanappe, and Luc Steels. 2009. Citizen noise pollution\n                monitoring.\n              </b>\n              In{' '}\n              <i>\n                Proceedings of the 10th Annual International Conference on\n                Digital Government Research: Social Networks: Making Connections\n                between Citizens, Data and Government\n              </i>{' '}\n              (<i>dg.o '09</i>). Digital Government Society of North America,\n              96–103. https://dl.acm.org/doi/10.5555/1556176.1556198\n            </p>\n            <Collapsible trigger=\"Read More\" className=\"full-text\">\n              <p className=\"full-text\">\n                In this paper, authors present a new approach to monitor noise\n                pollution involving citizens who can measure their personal\n                exposure to noise in their everyday environment by using\n                GPS-equipped mobile phones as noise sensors. The geo-localised\n                measures and user-generated meta-data can be automatically sent\n                and shared online with the public to contribute to the\n                collective noise mapping of cities. The prototype called Noise\n                Tube can be found online. This application collects local\n                information from different sensors (noise level, GPS\n                coordinates, time, user input) and sends them to the NoiseTube\n                server which visualizes the noise level data. The server\n                centralises and processes the data sent by the phones.\n                <br />\n                <br />\n                The mobile application contains a real-time signal processing\n                algorithm which measures the loudness level of the microphone\n                recording the environmental sound (at 22500 Hz, 16 bits) over 1\n                second at a chosen interval. On top of the sensing of the\n                loudness a real time visualization is displayed on the phone\n                with the decibels. To add meaning to this value it is associated\n                with a colour that represents the health risk of the current\n                exposure level: less than 70: green (no risk); between 70 and\n                80: yellow (be careful); more than 80: red (risky). See figure 2\n                below.\n                <br />\n                <br />\n                <img src={lit2} alt=\"poster gif\" className=\"vis-img\" />\n                <br />\n                In addition to measured loudness, the app allows to record the\n                source or context of noise, which is not always available but\n                remains important. Especially because the appreciation of sound\n                and loudness is a subjective matter – i.e. the perceived\n                annoyance (or pleasure) does not always correlate with its\n                loudness (see 6.2). Context is recorded through environmental\n                tagging (source of a noise e.g.: cars, aircraft, neighbours and\n                an annoyance rating/tag) and geo-tagging (gps positioning or\n                place tags (such as “home”, “work”, the name of the subway\n                station, ...) Geo-tagging feature we can reconstruct the\n                geo-coordinates afterwards notably for indoor locations (cf.\n                subway noise map in figure 3).\n                <br />\n                <img src={lit3} alt=\"poster gif\" className=\"vis-img\" />\n                <br />\n                <br />\n                Visualising Noise Maps. Once the measured data is sent to the\n                server, any user can see his own contributions or exposures by\n                going to the NoiseTube website and visualizing them on a map\n                thanks to Google Earth. The collective noise map is also\n                publicly available constructed by aggregating all the shared\n                participants. Each map can show a layer of participants to add\n                context and meaning to the loudness data. The authors also allow\n                users embed this as a web widget into their personal web pages\n                and provide publicly accessible web API to give full access to\n                third parties such scientists or developers can use individual\n                or collective exposure data to create web mash-ups or analyse\n                data for scientific purposes.\n              </p>\n            </Collapsible>\n            <br />\n          </div>\n          <div>\n            <p className=\"full-text\">\n              <b>\n                Silvia Santini, Benedikt Ostermaier, and Andrea Vitaletti. 2008.\n                First experiences using wireless sensor networks for noise\n                pollution monitoring.{' '}\n              </b>\n              In{' '}\n              <i>\n                Proceedings of the workshop on Real-world wireless sensor\n                networks\n              </i>{' '}\n              (<i>REALWSN '08</i>). Association for Computing Machinery, New\n              York, NY, USA, 61–65. DOI:https://doi.org/10.1145/1435473.1435490\n            </p>\n            <Collapsible trigger=\"Read More\" className=\"full-text\">\n              <p className=\"full-text\">\n                The authors focus on the assessment of environmental noise\n                pollution in urban areas and provide a feasibility analysis of\n                wireless sensor networks. They also present a prototype for the\n                collection and logging of noise pollution data based on the\n                Tmote invent prototyping platform, using which they performed\n                indoor and outdoor noise pollution measurements. They also\n                present tinyLAB, a Matlab-based tool developed in the context of\n                this work, which enables real-time acquisition, processing and\n                visualization of data collected in wireless sensor networks.\n                Authors mention that prototyping wireless sensor network\n                applications often requires visualizing the sensor data to\n                quickly identify any malfunctioning. For example, figure 1 shows\n                the responses to these acoustic events of four different nodes,\n                clearly pointing out a misalignment in the measured equivalent\n                noise levels. <br />\n                <br />\n                <img src={lit4} alt=\"poster gif\" className=\"vis-img\" />\n                <br />\n                <br />\n                Additionally, authors mention that current tools often do not\n                provide satisfactory data processing and visualization features\n                and propose using Matlab as it serves scientists in managing,\n                processing and visualizing their data and appears therefore\n                particularly well-suited to be used in the context of wireless\n                sensor networks. Authors develop tinyLAB, a simple framework\n                that allows to receive and send messages from and to a sensor\n                network and to visualize and process data as it comes from the\n                network. tinyLAB is implemented relying solely on the Matlab\n                software suite and offers a simple API to receive and send data.\n              </p>\n            </Collapsible>\n            <br />\n          </div>\n          <div>\n            <p className=\"full-text\">\n              <b>\n                Ann-sofie Gunnarsson, Malinda Rauhala, Anders Henrysson, and\n                Anders Ynnerman. 2006. Visualization of sensor data using mobile\n                phone augmented reality.{' '}\n              </b>\n              In{' '}\n              <i>\n                Proceedings of the 5th IEEE and ACM International Symposium on\n                Mixed and Augmented Reality\n              </i>{' '}\n              (<i>ISMAR '06</i>). IEEE Computer Society, USA, 233–234.\n              DOI:https://doi.org/10.1109/ISMAR.2006.297820\n            </p>\n            <Collapsible trigger=\"Read More\" className=\"full-text\">\n              <p className=\"full-text\">\n                The authors developed a prototype system for visual inspection\n                of hidden structures using a mobile phone wireless ZigBee sensor\n                network. Data collected from an embedded wireless sensor matrix\n                is used to synthesize AR visualizations in real-time. The AR\n                visualization is providing the user with an instant insight\n                concerning the status of the element being augmented. The\n                authors arrange sensors in a grid (e.g. a matrix), which allows\n                to take an approach where individual sensors emerge as pixels in\n                an image when their data is translated into color values. See\n                below an image that shows a mobile application overview.\n                <br />\n                <img src={lit5} alt=\"poster gif\" className=\"vis-img\" />\n                <br /> <br />\n                Authors argue that AR is an ideal way to present such context\n                related visualizations since it eliminates the focus switching\n                between the visualization domain (image) and problem domain\n                (real world). The sensors measure the relative humidity (RH) at\n                the location of the sensor, providing with data from a discrete\n                set of measure points in 3D. The values between the measure\n                points are interpolated creating a continuous visualization\n                which provides the user an overview of the humidity values as\n                well as their distribution. The mobile phone application\n                contains a visualization engine and a communication layer. The\n                interpolation is performed in real time and every time a new\n                sensor value is retrieved from the sensor network the\n                visualization is updated. Two visualization options are\n                implemented, one fully continuous, see Figure below, while the\n                other is composed of small quadratic units, separated using full\n                transparency. Authors believe that the user experiences a better\n                sense of orientation when less background information is covered\n                by the visualization.\n                <br />\n              </p>\n            </Collapsible>\n            <br />\n          </div>\n        </div>\n        <br /> <br />\n        <div className=\"block\">\n          <h4 className=\"gif-text\">Sensor Data Visualization Challenges </h4>\n          <p className=\"full-text\">\n            Visualizing large amounts of temporal data requires balancing the\n            goals of achieving high performance and interactivity. One solution\n            lies in intelligently aggregating the data to higher granularities,\n            so that the number of data points to be visualized is reduced and is\n            easier for the user to interpret - view image below. <br />\n            <img src={lit1} alt=\"poster gif\" className=\"vis-img\" />\n            <br />\n            <br />\n            Moreover, visualizing the longitudinal data such as noise or air\n            pollution data is challenging due to continuity of the data (no\n            precise start and end). For example, Many line or bar charts that\n            deal with the 24-hour cycle simply pick a point at which the chart\n            starts and ends. Sometimes the charts go from 12am-12am, sometimes\n            they use ranges like 4am-4am (which puts the break during a time\n            when most people are sleeping). For specific data this is often\n            acceptable, but in general it is a limitation (How can you pick an\n            arbitrary time to break the data? How are you sure the most\n            interesting part of the data doesn’t overlap when the chart begins\n            and ends?)\n          </p>\n          <br />\n        </div>\n        <br /> <br />\n        <div className=\"block\">\n          <h4 className=\"gif-text\">Visualization Graphs and Discussion </h4>\n          <h6 className=\"gif-text-h6\">Line Chart </h6>\n          <p className=\"full-text\"></p>\n          <br />\n        </div>\n        <br />\n      </div>\n      <Footer />\n    </>\n  )\n}\n\nexport default DataVisualization\n"]},"metadata":{},"sourceType":"module"}